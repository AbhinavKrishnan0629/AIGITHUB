Setting tensorflow GPU
----------------------
1) Download a GPU driver

2) Download cuDNN library of version supported by GPU driver

3) Download CUDA toolkit of version supported by cuDNN installed

4) create a tools folder in drive c, and add CUDA files to it and add this path to system variables

5) add paths : extras, cupti lib,
               include
               of the folders of CUDA toolkit
               

------------------------

using gpu

with tf.device("device name"):



-> tf.config.set_soft_device_placement(True): Allows managing the devices: cpu and gpu for different operations.    
   This can be costly since movement of data over devices consumes time.

-> tf.debuggin.set_log_device_placement(True):
   logs device placement(which operation is done on which device(CPU OR GPU)
   works on 1) Eager op execution  2) Graph construction
   not all logged ops are guaranteed to be executed at runtime due to Grappler(optimizer of tf.graphs)
   it doesnt fully work on TPU's currently.
    
    
-> Difference between eager execution and graph execution:

  Feature                    Eager Execution                                  Graph Execution
Execution Mode    Imperative (operations executed eagerly)    Declarative (operations executed within a graph)
Control Flow      Python control flow statements directly     Graph control flow constructs (e.g., tf.cond())
Debugging         Easier debugging                            Debugging can be more complex with session graphs
Optimization      Limited optimization opportunities          Allows for optimizations like constant folding, etc.
Deployment        Generally not used for deployment           Suitable for deployment as saved computational graph
Default Mode      TensorFlow 2.x default mode                 Supported but not the default in TensorFlow 2.x
    
    
-> Training API's 

Training API                        Advantages                                Disadvantages

Low-Level TensorFlow 
                 Maximum flexibility and control                    Requires writing more boilerplate code
                 Direct access to core TensorFlow functionalities   Steeper learning curve
                 Suitable for implementing custom algorithms/models Less user-friendly compared to higher-level APIs
Keras API        
                 Simple and intuitive interface                     Less flexibility compared to low-level TensorFlow
                 Abstracts away many implementation details         May not support certain advanced features
                 Wide range of pre-built components                 May not be as efficient for building complex model
Estimator API
                 High-level interface for training/deployment       Less flexibility compared to low-level TensorFlow
                 Supports distributed training                      Limited support for certain advanced features
                 Pre-made Estimators for common model types         May be less user-friendly for rapid prototyping
tf.data API 
                 Simple and efficient data input/preprocessing      Not specifically a training API
                 High performance and scalability                   Requires additional code for complex pipelines
                 Seamless integration with other TensorFlow APIs    Learning curve for developers not familiar with API
Custom Training Loops
                 Maximum flexibility and control                    Requires extensive manual coding
                 Enables implementation of advanced techniques      Steeper learning curve
                 Suitable for research or specialized use cases     May be less efficient or scalable for certain tasks
    
    
    
    
Strategies in tensorflow:
Each strategy has its own ways to train a model and update its parameters and weights.

1) MirroredStrategy (-0):
        is a distributed training strategy in TensorFlow that is primarily used for synchronous data parallelism. It's         a part of the tf.distribute.Strategy API, which allows you to distribute training across multiple GPUs on a             single machine. Mirrored Strategy works by replicating the model's variables (i.e., weights and biases) across         all available GPUs and ensuring that each GPU processes a portion of the input data. After each batch of data           is processed, the gradients are calculated independently on each GPU and then averaged across all GPUs. These           average gradients are then used to update the model's variables synchronously.
    
2) TPU Strategy(-1):
         is a distributed training strategy specifically designed for training machine learning models on Google Cloud          TPUs (Tensor Processing Units). TPUs are custom-developed ASICs (application-specific integrated circuits)              designed by Google for accelerating machine learning workloads.
         TPUStrategy allows you to efficiently utilize TPUs for training your models in a distributed manner. It works          similarly to other distributed training strategies in TensorFlow, such as MirroredStrategy, but is optimized            to take advantage of the unique architecture and capabilities of TPUs.
    
3) MultiWorkerMirroredStrategy(-2):
         is a distributed training strategy that allows you to train machine learning models across multiple devices or          machines in a synchronous data-parallel manner. It is primarily used for distributed training across multiple          workers, where each worker has one or more GPUs.
    
    
4) Central Storage strategy(-3):
         All variables and values will be stored in CPU and all operations will be performed in GPU's.

5) Parameter Server strategy(-4):
          is commonly used in distributed training scenarios where the model size exceeds the memory capacity of                 individual devices or when training requires fine-grained control over parameter updates. It provides                   scalability and flexibility for training large-scale machine learning models across distributed environments.
    


supported        :s
experimental     :e
limited support  :l
not supported    :n

Training API      (-0)      (-1)     (-2)    (-3)    (-4)
    
Keras model.fit    s          s       s        e       e

custom training    s          s       s        e       e
loop
    
Estimator API      l          n       l        l       l
    





























































