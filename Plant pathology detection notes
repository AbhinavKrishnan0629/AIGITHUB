Setting tensorflow GPU
----------------------
1) Download a GPU driver

2) Download cuDNN library of version supported by GPU driver

3) Download CUDA toolkit of version supported by cuDNN installed

4) create a tools folder in drive c, and add CUDA files to it and add this path to system variables

5) add paths : extras, cupti lib,
               include
               of the folders of CUDA toolkit
               

------------------------

using gpu

with tf.device("device name"):



-> tf.config.set_soft_device_placement(True): Allows managing the devices: cpu and gpu for different operations.    
   This can be costly since movement of data over devices consumes time.

-> tf.debuggin.set_log_device_placement(True):
   logs device placement(which operation is done on which device(CPU OR GPU)
   works on 1) Eager op execution  2) Graph construction
   not all logged ops are guaranteed to be executed at runtime due to Grappler(optimizer of tf.graphs)
   it doesnt fully work on TPU's currently.
    
    
-> Difference between eager execution and graph execution:

  Feature                    Eager Execution                                  Graph Execution
Execution Mode    Imperative (operations executed eagerly)    Declarative (operations executed within a graph)
Control Flow      Python control flow statements directly     Graph control flow constructs (e.g., tf.cond())
Debugging         Easier debugging                            Debugging can be more complex with session graphs
Optimization      Limited optimization opportunities          Allows for optimizations like constant folding, etc.
Deployment        Generally not used for deployment           Suitable for deployment as saved computational graph
Default Mode      TensorFlow 2.x default mode                 Supported but not the default in TensorFlow 2.x
    
    
-> Training API's 

Training API                        Advantages                                Disadvantages

Low-Level TensorFlow 
                 Maximum flexibility and control                    Requires writing more boilerplate code
                 Direct access to core TensorFlow functionalities   Steeper learning curve
                 Suitable for implementing custom algorithms/models Less user-friendly compared to higher-level APIs
Keras API        
                 Simple and intuitive interface                     Less flexibility compared to low-level TensorFlow
                 Abstracts away many implementation details         May not support certain advanced features
                 Wide range of pre-built components                 May not be as efficient for building complex model
Estimator API
                 High-level interface for training/deployment       Less flexibility compared to low-level TensorFlow
                 Supports distributed training                      Limited support for certain advanced features
                 Pre-made Estimators for common model types         May be less user-friendly for rapid prototyping
tf.data API 
                 Simple and efficient data input/preprocessing      Not specifically a training API
                 High performance and scalability                   Requires additional code for complex pipelines
                 Seamless integration with other TensorFlow APIs    Learning curve for developers not familiar with API
Custom Training Loops
                 Maximum flexibility and control                    Requires extensive manual coding
                 Enables implementation of advanced techniques      Steeper learning curve
                 Suitable for research or specialized use cases     May be less efficient or scalable for certain tasks
    
    
    
    
Strategies in tensorflow:
Each strategy has its own ways to train a model and update its parameters and weights.

1) MirroredStrategy (-0):
        is a distributed training strategy in TensorFlow that is primarily used for synchronous data parallelism. It's         a part of the tf.distribute.Strategy API, which allows you to distribute training across multiple GPUs on a             single machine. Mirrored Strategy works by replicating the model's variables (i.e., weights and biases) across         all available GPUs and ensuring that each GPU processes a portion of the input data. After each batch of data           is processed, the gradients are calculated independently on each GPU and then averaged across all GPUs. These           average gradients are then used to update the model's variables synchronously.
    
2) TPU Strategy(-1):
         is a distributed training strategy specifically designed for training machine learning models on Google Cloud          TPUs (Tensor Processing Units). TPUs are custom-developed ASICs (application-specific integrated circuits)              designed by Google for accelerating machine learning workloads.
         TPUStrategy allows you to efficiently utilize TPUs for training your models in a distributed manner. It works          similarly to other distributed training strategies in TensorFlow, such as MirroredStrategy, but is optimized            to take advantage of the unique architecture and capabilities of TPUs.
    
3) MultiWorkerMirroredStrategy(-2):
         is a distributed training strategy that allows you to train machine learning models across multiple devices or          machines in a synchronous data-parallel manner. It is primarily used for distributed training across multiple          workers, where each worker has one or more GPUs.
    
    
4) Central Storage strategy(-3):
         All variables and values will be stored in CPU and all operations will be performed in GPU's.

5) Parameter Server strategy(-4):
          is commonly used in distributed training scenarios where the model size exceeds the memory capacity of                 individual devices or when training requires fine-grained control over parameter updates. It provides                   scalability and flexibility for training large-scale machine learning models across distributed environments.
    


supported        :s
experimental     :e
limited support  :l
not supported    :n

Training API      (-0)      (-1)     (-2)    (-3)    (-4)
    
Keras model.fit    s          s       s        e       e

custom training    s          s       s        e       e
loop
    
Estimator API      l          n       l        l       l
    


-> Optimization of single gpu is given in : tensorflow.org/guide/gpu_performance_analysis

-> Initially a very little GPU resource is allocated.
   As the need increases, GPU resource is allocated more to tensorflow process.
   Memory is not released as it would couse fragmentation.
   This is performed by seting memory growth by
       tf.config.experimental.set_memory_growth(gpu, True)
       where 'gpu' is name of the device: /device:GPU:0

-> We can set the limit of amount of gpu resource to be used by
       tf.config.set_logical_device_configuration(
                                                   gpus[0],
                                                   [tf.config.LogicalDeviceConfiguration(memory_limit=1024)]
                                                  )



-> tf.distribut.get_strategy()   will return the current strategy.


-> with strategy.scope():
        ----
        ----
        ----
   
   // the variables declared inside this scope will be done according to the strategy for distribution defined earlier.

What should be in scope and what should be outside?

There are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).

Anything that creates variables that should be distributed variables must be called in a strategy.scope. This can be accomplished either by directly calling the variable creating function within the scope context, or by relying on another API like strategy.run or keras.Model.fit to automatically enter it for you. Any variable that is created outside scope will not be distributed and may have performance implications. Some common objects that create variables in TF are Models, Optimizers, Metrics. Such objects should always be initialized in the scope, and any functions that may lazily create variables (e.g., Model.call(), tracing a tf.function, etc.) should similarly be called within scope. Another source of variable creation can be a checkpoint restore - when variables are created lazily. Note that any variable created inside a strategy captures the strategy information. So reading and writing to these variables outside the strategy.scope can also work seamlessly, without the user having to enter the scope.

Some strategy APIs (such as strategy.run and strategy.reduce) which require to be in a strategy's scope, enter the scope automatically, which means when using those APIs you don't need to explicitly enter the scope yourself.
When a tf.keras.Model is created inside a strategy.scope, the Model object captures the scope information. When high level training framework methods such as model.compile, model.fit, etc. are then called, the captured scope will be automatically entered, and the associated strategy will be used to distribute the training etc. See a detailed example in distributed keras tutorial. WARNING: Simply calling model(..) does not automatically enter the captured scope -- only high level training framework APIs support this behavior: model.compile, model.fit, model.evaluate, model.predict and model.save can all be called inside or outside the scope.
The following can be either inside or outside the scope:
Creating the input datasets
Defining tf.functions that represent your training step
Saving APIs such as tf.saved_model.save. Loading creates variables, so that should go inside the scope if you want to train the model in a distributed way.
Checkpoint saving. As mentioned above - checkpoint.restore may sometimes need to be inside scope if it creates variables.




























































