from sklearn import preprocessing

preprocessing.LabelEncoder

from sklearn.model_selection import train_test_split

model.predict_proba(xvalid)

from sklearn.linear_model import LogisticRegression

from sklearn.naive_bayes import MultinomialNB

from sklearn import decomposition

from sklearn.svm import SVC


NCE: Noise Contrastive Estimation
ReLu: Rectified Linear Unit
NER: Named Entity Recognition
CNN: Convolutional Neural Network
RNN: Recurrent Neural Network
RSS value: Residual Sum of Squares
RSE value: Residual Standard Error
RMSE value: Root Mean Square Error
TextFuseNet
=========================================================================================================================





Things to be covered:
-> Working of Latent Dirichlet Allocation, Non Negative Matrix Factorization, LabelEncoder
-> Stratify splitting
-> TF-IDF, Logistic regression
-> Parameters of TF-IDF, how to tune to get best settings for TF-IDF
-> What is TF-IDF followed by logistic regression means
-> What are linear models and non linear models
-> what is parameter C in logistic Regresion
-> Which model is better for which cases
-> Uses and strength areas of each model
-> How to choose combinations of models
-> What is Singular Value Decomposition
-> Things to consider while passing data to SVM
-> What is XGBOOST
-> Learn parameters of all models and how to implement in tensorflow
-> Why is the training time of SVM model takes so long
-> Full form of SVD
-> What is fasttext(GloVe, word2vec)
-> You can download the GloVe vectors from here http://www-nlp.stanford.edu/data/glove.840B.300d.zip


Things to consider to build a good pipeline
-> Data type consistancy, make sure you are dealing with same datatype
-> After reading a file and updating to dataset(refer ch3 word to vec skip gram alg, def builddataset(words) to reduce    memory usage
-> Loss alone is not a good metric of accuracy as it might occur due to overfitting.
   Word analogy test can be a good metric.
   More about word analogy test :
-> Visualisation of higher dim data to lower dim data can be done with the help of TensorBoard
   tensorboard_word_embeddings.ipynb1
-> CBOW works better in synctactic tasks and skip gram works better in semantic tasks.
-> Skip gram algorithm works better than CBOW for larger dataset


Links
-> Google analogy dataset: http://download.
   tensorflow.org/data/questions-words.txt
-> Bigger Analogy Test Set (BATS): http://vsm.
   blackbird.pw/bats



Topics covered:
-> tfidf, count features, logistic regression, svm, xgboost, naive bayes, grid search, word vectors, LSTM,
   GRU, ensembling

-> Always try to start with TF-IDF and CountVectorizer features, they work almost all the time.

-> Hyperparameter optimization

Grid Search





























